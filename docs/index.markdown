---
layout: default
---

> The goal of the project was to partially reproduce some of the ideas, analyses, and results from an original published paper, Antony et al., 2020 (**[WebNLG](https://www.biorxiv.org/content/10.1101/2020.03.26.008714v2.full).**) Through second-level analysis of 20 subjects, fMRI data that was used in the original paper was analyzed to determine if there is significant region activation during viewing and recall tasks.

# Introduction

Prompt tuning and prefix tuning are two effective mechanisms to leverage frozen language models to perform downstream tasks. Robustness reflects models' resilience of output under a change or noise in the input. In this project, we analyze the robustness of natural language models using various tuning methods with respect to a domain shift (i.e. training on a domain but evaluating on out-of-domain data). We apply both prompt tuning and prefix tuning on T5 models for reading comprehension (i.e. question-answering) and GPT-2 models for table-to-text generation.

# Methods
### Sub-Heading

# Results
**We have four findings:**

# Conclusion

# References
